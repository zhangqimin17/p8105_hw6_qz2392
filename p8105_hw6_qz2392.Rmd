---
title: "p8105_hw6_qz2392"
author: "Qimin Zhang"
date: "11/20/2019"
output: 
  github_document:
    pandoc_args: --webtex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(modelr)
library(tidyselect)
set.seed(2019)
```

# Problem 1

```{r message=FALSE, warning=FALSE}
birthweight = 
  read_csv("./birthweight.csv") %>% 
  janitor::clean_names() %>% 
  mutate(
    babysex = as.factor(babysex),
    frace = as.factor(frace),
    malform = as.factor(malform),
    mrace = as.factor(mrace)
  )
```

```{r}
reg = lm(bwt ~ ., data = birthweight) %>%
  step(direction = "backward")

summary(reg)
```

This model is generated by stepwise regression with 'backward' option. It starts with all predictors in the model and then remove the predictor with the highest p-value > $\alpha_{critical}$. Then re-fit the model and remove the next least significant predictor. Done when all non-significant predictors have been removed.

```{r}
birthweight %>% 
  add_predictions(reg) %>% 
  add_residuals(reg) %>% 
  ggplot(aes(x = pred, y = resid)) +
  geom_point(alpha = .3) +
  labs(
    titles = "Residuals VS fitted values"
  )
```

The plot is centered around 0 with some outliers on the left and top-left.

```{r}
lm(bwt ~ blength + gaweeks, data = birthweight) %>% 
 summary()
```

This model is with less variables and smaller adjusted R-squared.

```{r}
lm(bwt ~ bhead * blength * babysex, data = birthweight) %>% 
  summary()
```

This model had a smaller adjusted R-squared than mine but with statistically significant interaction term.

Use cross validation to compare these three models.

```{r}
cv_df = 
  crossv_mc(birthweight, 30) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
    )
```

```{r message=FALSE, warning=FALSE, include=FALSE}
cv_df = 
  cv_df %>% 
  mutate(reg1  = map(train, ~lm(bwt ~ ., data = .x) %>% step(direction = "backward")),
         reg2  = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
         reg3  = map(train, ~lm(bwt ~ bhead * blength * babysex, data = .x))) %>% 
  mutate(rmse1 = map2_dbl(reg1, test, ~rmse(model = .x, data = .y)),
         rmse2 = map2_dbl(reg2, test, ~rmse(model = .x, data = .y)),
         rmse3 = map2_dbl(reg3, test, ~rmse(model = .x, data = .y)))
```

```{r}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin()
```

According to the violin plot, my model, i.e., model 1 had the best performance.

# Problem 2

```{r message=FALSE, warning=FALSE}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

```{r}
boot_straps =
  weather_df %>% 
  bootstrap(n = 5000)
```

```{r}
boot_straps =
boot_straps %>% 
   mutate(models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
         results_coef = map(models, broom::tidy),
         results_glance = map(models, broom::glance)) %>% 
  select(-strap, -models)
```

Plot the distribution of $\hat r^2$.
```{r}
boot_straps %>% 
  unnest(results_glance) %>% 
  ggplot(aes(x = adj.r.squared)) +
  geom_density() +
  labs(
    title = "Density of R-squared"
  )
```

The distribution of $\hat r^2$ looks little bit left skewed, but generally normal with mean at around 0.913.

Plot the distribution of $log(\hat\beta_0 \times \hat\beta_1)$.
```{r}
boot_straps %>% 
  unnest_wider(results_coef) %>% 
  mutate(
    beta0 = map(estimate, ~ .[[1]]),
    beta1 = map(estimate, ~ .[[2]]),
    beta0 = as.numeric(beta0),
    beta1 = as.numeric(beta1),
    log_beta0_times_beta1 = log(beta0*beta1)
  ) %>% 
  ggplot(aes(x = log_beta0_times_beta1)) +
  geom_density() +
  labs(
    title = "Density of log(beta0_hat*beta1_hat)"
  )
```

The distribution of $log(\hat\beta_0 \times \hat\beta_1)$ looks little bit left skewed, but generally normal with mean at around 2.01.

```{r}
boot_straps %>% 
  unnest(results_glance) %>%
  pull(adj.r.squared) %>% 
  as.vector() %>% 
  quantile(c(0.025, 0.975))
```

The 95% confidence interval for $\hat r^2$ is [0.894, 0.927].

```{r}
boot_straps %>% 
  unnest_wider(results_coef) %>% 
  mutate(
    beta0 = map(estimate, ~ .[[1]]),
    beta1 = map(estimate, ~ .[[2]]),
    beta0 = as.numeric(beta0),
    beta1 = as.numeric(beta1),
    log_beta0_times_beta1 = log(beta0*beta1)
  ) %>% 
  pull(log_beta0_times_beta1) %>% 
  as.vector() %>% 
  quantile(c(0.025, 0.975))
```

The 95% confidence interval for $log(\hat\beta_0 \times \hat\beta_1)$ is [1.965, 2.059].